{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HTML Refresher\n",
    "This part is based on chapter 11 of *Automate the Boring Stuff with Python* by Al Sweigart\n",
    "\n",
    "HTML files are plain text files containing *tags*, which are words enclosed in angle brackets. Tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags.\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the `<a>` tag encloses text that should be a link.\n",
    "\n",
    "Some elements have an `id` attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Couldn't find program: 'bash'\n"
    }
   ],
   "source": [
    "%%bash\n",
    "cat << EOF > example.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Hello!</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Hello World!</h1>\n",
    "You are extremely welcome!<br>\n",
    "<br>\n",
    "The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\">Lecture Notes</a>.<br>\n",
    "<div>\n",
    "<p>paragraph 1</p>\n",
    "<p>and paragraph 2: <span id=\"span01\">This is span 1</span id=\"span02\"><span id=\"span03\">Second span element</span>\n",
    "<span class=\"red_border\">Here is the third span</span>\n",
    "</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Couldn't find program: 'bash'\n"
    }
   ],
   "source": [
    "%%bash\n",
    "xdg-open example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# View a Page's HTML Sources\n",
    "\n",
    "Here, I will only describe how to use Firefox' development features.\n",
    "\n",
    "To view a page's sources right click on it and choose **View page source** which opens a new tab with the HTML sources.\n",
    "\n",
    "<img src=\"images/view_source_small.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Firefox, you can bring up the Web Developer Tools Inspector by pressing `CTRL-SHIFT-C` on Windows and Linux or by `CMD-OPTION-C` on OS X.\n",
    "\n",
    "<img src=\"images/inspector_small.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a module for parsing and extracting information from HTML sources. The module’s name is bs4. In case it is not already installed on your machine:\n",
    "- install it with `pip install beautifulsoup4`. While beautifulsoup4 is the name used for installation, \n",
    "- to import BeautifulSoup you have to use `import bs4`.\n",
    "\n",
    "According to its documentation (https://www.crummy.com/software/BeautifulSoup/) *\"Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Local HTML File\n",
    "\n",
    "- The `bs4.BeautifulSoup()` function needs to be called with a string containing the HTML it will parse. \n",
    "- The `bs4.BeautifulSoup()` function returns is a `BeautifulSoup` object.\n",
    "\n",
    "You can load a local HTML file and pass a file object to `bs4.BeautifulSoup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './example.html'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dae68adf7682>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./example.html'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mexample_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './example.html'"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html)\n",
    "print(type(soup))\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Remote HTML File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://github.com/datsoftlyngby/dat4sem2020spring-python')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve HTML elements from a `BeautifulSoup` object by calling the `select()` method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: They specify a pattern to look for, in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "Common CSS selector patterns include:\n",
    "\n",
    "  * `soup.select('div')` ... selects all elements named `<div>`\n",
    "  * `soup.select('#lecturer')`  ... selects the element with an id attribute of author\n",
    "  * `soup.select('.notice')` ... selects all elements that use a CSS class attribute named notice\n",
    "  * `soup.select('div span')` ... selects all elements named `<span>` that are within an element named `<div>`\n",
    "  * `soup.select('div > span')` ... selects all elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "  * `soup.select('input[name]')` ... selects all elements named `<input>` that have a name attribute with any value\n",
    "  * `soup.select('input[type=\"button\"]')` ... selects all elements named `<input>` that have an attribute named type with value button\n",
    "  \n",
    "See more in the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "\n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "\n",
    "elems = soup.select('body')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print('1: return type of select()',type(elems))\n",
    "print('2: length of the returned list',len(elems))\n",
    "print('3: type of elements in the list',type(elems[0]))\n",
    "print('4: get text from the element',elems[0].getText()[:40])\n",
    "print('5: string representation of an element: ',str(elems[0]))\n",
    "print('6: the attributes of the element: ',elems[0].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_elems = soup.select('p')\n",
    "\n",
    "for el in p_elems:\n",
    "    print(str(el))\n",
    "    print(el.getText())\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Data from an Element’s Attributes\n",
    "\n",
    "The `get()` method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'id': 'lecturer'}.get('id', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "# soup.find_all?\n",
    "span_elem = soup.select('span')[0]\n",
    "print(str(span_elem))\n",
    "print(span_elem.get('id'))\n",
    "print(span_elem.get('some_nonexistent_addr') == None)\n",
    "print(span_elem.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the difference between the `select` and the `find`/`find_all` functions?\n",
    "\n",
    "You are not the first ones wondering about this... See:\n",
    "https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x#38033910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Scraping Events from a Page\n",
    "\n",
    "\n",
    "Ususally, you will use web scraping to collect information, which you cannot gather otherwise. \n",
    "For example, let's imagine we want to do some statistics about:\n",
    "- concerts in Copenhagen, \n",
    "- their start times and \n",
    "- their door prices.\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage www.kultunaut.dk, \n",
    "\n",
    "The website lists all possible events in Denmark. \n",
    "Concerts in Copenhagen are for example accessible here: \n",
    "- http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik\n",
    "\n",
    "**OBS** Many web pages are not built to support high traffic or they exlicitely discourage automatic access. Keep this in mind when writing your scraping tool.\n",
    "- from time import sleep\n",
    "- sleep(3) # sleep 3 seconds\n",
    "\n",
    "\n",
    "Considering our example:\n",
    "- we have to first figure out how many events there are at all. \n",
    "- We need this information, as events are given paginated, i.e., twenty events per page.\n",
    "- The link given above only returns the link to the first page with the first twenty events. \n",
    "- Out of the total amount of events we can generate the URLs for the subsequent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "elems = soup.find_all('td', attrs={'style': 'background-color:#CBDCEE;color:#324669'})[0]\n",
    "b_el = elems.find('b')\n",
    "print(b_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from text (the hard way)\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik')\n",
    "r.raise_for_status\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "elems = soup.find_all('td', {'style': 'background-color:#CBDCEE;color:#324669'})\n",
    "el0 = elems[0]\n",
    "\n",
    "fifth = str(el0).split(' ')[5]\n",
    "\n",
    "number = int(str(fifth.split('\\n')[0]))\n",
    "#print(el0)\n",
    "#print(fifth)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the browser inspector pane:\n",
    "\n",
    "<img src=\"images/inspect_element.png\" width=\"500\">\n",
    "\n",
    "We can see that the desired element is hiding in a structure like: a b-tag inside a h3-tag inside a td-tag or:\n",
    "- `('td h3 b')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use select with css-selectors rather than find_all\n",
    "import bs4\n",
    "import requests\n",
    "html = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut?Area=&ArrStartday=22&ArrStartmonth=Marts&ArrStartyear=2020&ArrSlutday=29&ArrSlutmonth=Marts&ArrSlutyear=2020&ArrMaalgruppe=&DefaultGenre=Musik&ArrKunstner=')\n",
    "txt = html.text\n",
    "soup = bs4.BeautifulSoup(txt, 'html.parser')\n",
    "events = soup.select('td h3 b')\n",
    "print('number of events is {}'.format(len(events)))\n",
    "for e in events:\n",
    "    print(e.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the links in a document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "gov = requests.get('https://analytics.usa.gov')\n",
    "soup = BeautifulSoup(gov.text, 'lxml')\n",
    "print(soup.prettify()[:100])\n",
    "print('------------------------')\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using elems.text to get a string from the bs4.element.Tag object\n",
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "elems = soup.find_all('td', attrs={'style': 'background-color:#CBDCEE;color:#324669'})[0].find('b')\n",
    "print(elems.text, type(elems.text), type(elems))\n",
    "no_events = elems.text.split(' ')[3].split('\\r\\n')[0]\n",
    "no_events = int(no_events)\n",
    "print('Events: ',no_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we can scrape the events per page. Observe, that now, out `base_url` http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik& has a placeholder for the paginated results (`Startnr=`).\n",
    "\n",
    "Consequently, we scrape each page separately, see the function on the next slide: `scrape_events_per_page`. From examining the page's source code, we know that events are all given as table entries with a corresponding header. We iterate over each of the table cells and extract the strings for dates and prices if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def scrape_events_per_page(url):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        A list of tuples of strings holding title, place, date, and price\n",
    "        for concerts in Copenhagen scraped from Kulturnaut.dk\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    event_cells = soup.find_all('td', {'align' : 'top'})\n",
    "    #print('size',len(event_cells))\n",
    "    scraped_events_per_page = []\n",
    "    \n",
    "    for event_cell in event_cells:\n",
    "        try:\n",
    "            title = event_cell.find('b').text\n",
    "            spans = event_cell.find_all('span')\n",
    "            place = spans[3].text\n",
    "            try:\n",
    "                date, price = spans[0].text.splitlines()\n",
    "            except ValueError as e:\n",
    "                date = spans[0].text.splitlines()[0]\n",
    "                price = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        scraped_events_per_page.append((title, place, date, price))\n",
    "        \n",
    "    return scraped_events_per_page\n",
    "\n",
    "\n",
    "base_url = 'http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik&'\n",
    "\n",
    "scraped_events = []\n",
    "indexes = list(range(1, no_events, 20))\n",
    "indexes[0] = 0\n",
    "\n",
    "for idx in tqdm(indexes):\n",
    "    scrape_url = base_url.format(idx)\n",
    "    #print(scrape_url)\n",
    "    scraped_events += scrape_events_per_page(scrape_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = scrape_events_per_page('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr=1481&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik&')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do we have so far?\n",
    "\n",
    "Now, you can see that we extracted a list of four element string tuples consisting of the title of the event, its location, a date and a time, and an entrance fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('The Marcus King Band (us) - Udsolgt.',\n  'VEGA, Enghavevej 40, Copenhagen V',\n  'Monday 9 March 2020, 8 pm.',\n  '(Entrance fee: 225 DKK)'),\n ('Ginne Marker.',\n  'Jazzhus Montmartre, Store Regnegade 19A, Copenhagen K',\n  'Monday 9 March 2020, 8 pm.',\n  '(Entrance fee: 110 DKK + evt. gebyr)'),\n ('Med på Noderne.',\n  'Drop Inn, Kompagnistræde 34, Copenhagen K',\n  'Monday 9 March 2020, 8 pm.',\n  '(Free admission)'),\n ('Princess nokia (US).',\n  'Pumpehuset, Studiestræde 52, Copenhagen V',\n  'Monday 9 March 2020, 8 pm.',\n  '(Entrance fee: 290 DKK)'),\n ('Baby lydplaneter.',\n  'Osramhuset, Valhalsgade 4, Copenhagen N',\n  'Tuesday 10 March 2020, 12 am.',\n  '(Entrance fee: 50 DKK + Gebyr)'),\n ('Stemme og Chakra Frekvenser: Solar plexus.',\n  'Metronomen, Godthåbsvej 33, Frederiksberg',\n  'Tuesday 10 March 2020, 7 pm.',\n  '(Entrance fee: 50 DKK)'),\n ('Dr Pigekorets Forår Udsolgt.',\n  'Holmens Kirkegård, Dag Hammarskjölds Allé 35, Copenhagen Ø',\n  'Tuesday 10 March 2020, 7.30 pm.',\n  '(Entrance fee: 210 DKK)'),\n ('Ginne Marker.',\n  'Jazzhus Montmartre, Store Regnegade 19A, Copenhagen K',\n  'Tuesday 10 March 2020, 8 pm.',\n  '(Entrance fee: 110 DKK + evt. gebyr)'),\n ('Simple Minds 2020 - 40th Anniversary Tour - Udsolgt/venteliste.',\n  'VEGA, Enghavevej 40, Copenhagen V',\n  'Tuesday 10 March 2020, 8 pm.',\n  '(Entrance fee)'),\n ('Little Dragon (se) - Udsolgt.',\n  'VEGA, Enghavevej 40, Copenhagen V',\n  'Tuesday 10 March 2020, 8 pm.',\n  '(Entrance fee: 255 DKK)'),\n ('Crazy Ivans.',\n  'Kulturhuset Pilegården, Brønshøjvej 17, Brønshøj',\n  'Tuesday 10 March 2020, 8 pm.',\n  '(Entrance fee: 140 DKK)'),\n ('Torres (US) + Harkin (UK).',\n  'Loppen, Christiania, Bådsmandsstræde 43, Copenhagen K',\n  'Tuesday 10 March 2020, 8.30 pm.',\n  '(Entrance fee: 140 DKK)'),\n ('Moseholm SuperBand.',\n  'Huset KBH - Stardust, Rådhusstræde 13, 1. tv., Copenhagen K',\n  'Tuesday 10 March 2020, 9 pm  - 11.30 pm.',\n  '(Entrance fee: 150 DKK + gebyr)'),\n ('Morgensang på Hovedbiblioteket #233.',\n  'Københavns Hovedbibliotek, Krystalgade 15, Copenhagen K',\n  'Wednesday 11 March 2020, 8.30 am  - 8.50 am.',\n  ''),\n ('Klassiske minikoncerter for små lyttere (0-2 år).',\n  'K-I-B, Kulturhuset Islands Brygge, Islands Brygge 18, Copenhagen S',\n  'Wednesday 11 March 2020, 1 pm.',\n  '(Entrance fee: 40 DKK pr. voksen og pr. barn)'),\n ('Klassiske minikoncerter for små lyttere (0-2 år).',\n  'K-I-B, Kulturhuset Islands Brygge, Islands Brygge 18, Copenhagen S',\n  'Wednesday 11 March 2020, 2 pm.',\n  '(Entrance fee: 40 DKK pr. voksen og pr. barn)'),\n ('18. novembers sangkor.',\n  'Helligaandshuset, Valkendorfsgade  23, 1. sal, Copenhagen K',\n  'Wednesday 11 March 2020, 4 pm.',\n  '(Free admission)'),\n ('Fyraftenskoncert - klaver og oplæsning ved Maria Bundgård.',\n  'Sions Kirke, Østerbrogade 192, Copenhagen Ø',\n  'Wednesday 11 March 2020, 5 pm.',\n  ''),\n ('Aftenmusik i fastetiden v/ organist Lars Sømod.',\n  'Vor Frelsers Kirke, Sankt Annæ Gade 29, Copenhagen K',\n  'Wednesday 11 March 2020, 5.30 pm.',\n  ''),\n ('Ung Jam.',\n  'Drop Inn, Kompagnistræde 34, Copenhagen K',\n  'Wednesday 11 March 2020, 7 pm.',\n  '(Free admission)')]"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "scraped_events[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise with beautifulSoup\n",
    "Use BeautifulSoup to extract all titles on all radio programs https://www.dr.dk/radio/programmer\n",
    "1. First find how many pages there are\n",
    "2. Then find all titles on https://www.dr.dk/radio/programmer?side=1\n",
    "3. Then find all titles on all pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-fa866ef8ee31>, line 3)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-fa866ef8ee31>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### How to Extract Dates and Prices from Strings.\n",
    "\n",
    "Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\n",
    "\n",
    "\n",
    "Since dates given on the web do not necessarily conform to standardized time formats, we can apply the `dateparser` (https://pypi.python.org/pypi/dateparser) module, which tries to parse arbitrary strings into datetimes.\n",
    "\n",
    "You can install the module via:\n",
    "\n",
    "```bash\n",
    "pip install dateparser\n",
    "```\n",
    "\n",
    "You can read more about the module and its capabilities https://dateparser.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Extract Dates and Prices from Strings.\n",
    "\n",
    "Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\n",
    "\n",
    "\n",
    "Since dates given on the web do not necessarily conform to standardized time formats, we can apply the `dateparser` (https://pypi.python.org/pypi/dateparser) module, which tries to parse arbitrary strings into datetimes.\n",
    "\n",
    "You can install the module via:\n",
    "\n",
    "```bash\n",
    "pip install dateparser\n",
    "```\n",
    "\n",
    "You can read more about the module and its capabilities https://dateparser.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dateparser'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-aded81ecfd4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdateparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dateparser'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from dateparser import parse\n",
    "\n",
    "\n",
    "def get_dates_and_prices(scraped_events):\n",
    "    \"\"\"\n",
    "    Cleanup the data. Get price as integer and date as date.\n",
    "    \n",
    "    returns:\n",
    "        A two-element tuple with a datetime representing the start \n",
    "        time of an event and an integer representing the price in Dkk.\n",
    "    \"\"\"\n",
    "\n",
    "    price_regexp = r\"(?P<price>\\d+)\" #initial ? is a lookbehind. r() r is for raw text, P<some pattern> is to give a pattern name to refer to. \\d is numeric digit, + is for 1 or more.\n",
    "\n",
    "    data_points = []\n",
    "    three_at_night = datetime.now().replace(hour=3, minute=0, second=0, microsecond=0).time()\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str, price_str = event_data\n",
    "        \n",
    "        if 'Free admission' in price_str:\n",
    "            price = 0\n",
    "        else:\n",
    "            m = re.search(price_regexp, price_str) # m is the Match object returned from re.search (might be None)\n",
    "            try:\n",
    "                price = int(m.group('price')) # if price can be converted to int then we do it else return 0.\n",
    "            except:\n",
    "                price = 0\n",
    "\n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date and date.time() > three_at_night:\n",
    "            data_points.append((date, price))\n",
    "            \n",
    "    return data_points\n",
    "\n",
    "\n",
    "dates_and_prices = get_dates_and_prices(scraped_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_and_prices[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plotting Times vs. Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dates, prices = zip(*dates_and_prices) # See zip example with * here: https://www.programiz.com/python-programming/online-compiler/?ref=2390fcb4\"\n",
    "print([d.time() for d in dates][:10])\n",
    "ref_day = datetime.today()\n",
    "times = [datetime.combine(ref_day, t.time()) for t in dates] # get the time intervals for today\n",
    "\n",
    "# plot\n",
    "plt.plot(times, prices, 'ro')\n",
    "#plt.plot(dates, prices, 'ro')\n",
    "# beautify the x-labels\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Pearson Correlation Coefficient\n",
    "\n",
    "The Pearson correlation coefficient measures the linear relationship\n",
    "between two datasets. Like other correlation coefficients, this one varies between -1 and +1\n",
    "with 0 implying no correlation. Correlations of -1 or +1 imply an exact\n",
    "linear relationship. Positive correlations imply that as x increases, so\n",
    "does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "The p-value roughly indicates the probability of an uncorrelated system\n",
    "producing datasets that have a Pearson correlation at least as extreme\n",
    "as the one computed from these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "x, y = zip(*dates_and_prices)\n",
    "x = matplotlib.dates.date2num(x)\n",
    "\n",
    "pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consequently, since our Pearson Correlation Coefficient is quite close to zero, there is likely no correlation between the start time of a concert and the price you have to pay for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class exercise: Scraping Images from a Page\n",
    "\n",
    "In the following code you will use Beautiful Soup to extract all links to images, which are in `img` tags on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "\n",
    "def collect_img_links(url):\n",
    "    \"\"\"based on a url returns a list of image links contained in the requested page\"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #print(soup.select('img'))\n",
    "    return [img.get('src') for img in soup.select('img') \n",
    "            if img.get('src') and img.get('src').startswith('http')]\n",
    "\n",
    "\n",
    "def download_imgs(links, out_folder=\"./test/\"):\n",
    "    \"\"\"download all images from a list of image links. \n",
    "    Requires a folder named: test to be there\"\"\"\n",
    "    img_no = 0\n",
    "    for l in links:\n",
    "        img_no += 1\n",
    "        r = requests.get(l, stream=True)\n",
    "        with open(out_folder+'img'+str(img_no)+'.jpg', 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     \n",
    "        \n",
    "links = collect_img_links('https://www.google.dk/search?site=&tbm=isch&source=hp&biw=1163&bih=812&q=minions&oq=minions')\n",
    "links\n",
    "download_imgs(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2: Writing a Simple Web Crawler\n",
    "\n",
    "Write a simple web crawler. More precisely, a program that extracts recursively all links from web pages. The result of running the web crawler is a dictionary, were the key-value pairs correspond to outgoiung links from a web page with the URL, which is stored in the key.\n",
    "\n",
    "\n",
    "In case a page returns a status code, which is not `200` we just disregard this page. See https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for more detailes on the various HTTP status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "    # This is what the exercise below asks you to implement!\n",
    "    pass\n",
    "\n",
    "\n",
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The web crawler that you wrote above is perhaps not the most performant. If you are interested in more web scraping and application of crawlers have a look at the `scrapy` module (https://scrapy.org)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}